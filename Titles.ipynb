{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install wordcloud\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('/Users/victoria/Final_Project_Books/data/covers/book_covers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "six = books[(books['category']=='Crime-Thriller')|(books['category']=='Graphic-Novels-Anime-Manga')|(books['category']=='Mind-Body-Spirit')|(books['category']=='Romance')|(books['category']=='Science-Fiction-Fantasy-Horror')|(books['category']=='Travel-Holiday-Guides')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "six['price'] = six['price'].replace('[A-Z$]','',regex=True)\n",
    "six['price'] = pd.to_numeric(six['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5921 entries, 7900 to 32580\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   image                  5921 non-null   object \n",
      " 1   name                   5921 non-null   object \n",
      " 2   author                 5915 non-null   object \n",
      " 3   format                 5920 non-null   object \n",
      " 4   book_depository_stars  5921 non-null   float64\n",
      " 5   price                  5921 non-null   float64\n",
      " 6   currency               5921 non-null   object \n",
      " 7   old_price              5172 non-null   float64\n",
      " 8   isbn                   5921 non-null   int64  \n",
      " 9   category               5921 non-null   object \n",
      " 10  img_paths              5921 non-null   object \n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 555.1+ KB\n"
     ]
    }
   ],
   "source": [
    "six.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "six = six.drop(['format', 'currency', 'old_price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "six.to_csv('/Users/victoria/Books/six-genres-to-analyse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frankenstein                          11\n",
       "The Picture of Dorian Gray            10\n",
       "Dracula                                7\n",
       "Batman                                 7\n",
       "Outlander                              7\n",
       "Eat, Pray, Love                        7\n",
       "Animal Farm                            7\n",
       "Ready Player One                       6\n",
       "A Column of Fire                       6\n",
       "The Martian                            6\n",
       "It                                     6\n",
       "The Hobbit                             6\n",
       "Fables                                 6\n",
       "The Haunting of Hill House             6\n",
       "Heroes                                 6\n",
       "Gone Girl                              6\n",
       "Nine Perfect Strangers                 6\n",
       "The Shining                            5\n",
       "American Psycho                        5\n",
       "We Have Always Lived in the Castle     5\n",
       "The Man in the High Castle             5\n",
       "The Silmarillion                       5\n",
       "The Outsider                           5\n",
       "Monster                                5\n",
       "Wild                                   5\n",
       "Good Omens                             5\n",
       "World War Z                            5\n",
       "A Discovery of Witches                 4\n",
       "A Darker Shade of Magic                4\n",
       "After                                  4\n",
       "Still Me                               4\n",
       "Doctor Sleep                           4\n",
       "A Game of Thrones                      4\n",
       "The Lord of the Rings                  4\n",
       "The Ministry of Utmost Happiness       4\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six['name'].value_counts().head(35) #many are repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Six dataframes for each genre and wordclouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = six[(six['category']=='Crime-Thriller')]\n",
    "graph = six[(six['category']=='Graphic-Novels-Anime-Manga')]\n",
    "mind = six[(six['category']=='Mind-Body-Spirit')]\n",
    "romance = six[(six['category']=='Romance')]\n",
    "science = six[(six['category']=='Science-Fiction-Fantasy-Horror')]\n",
    "travel = six[(six['category']=='Travel-Holiday-Guides')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def clean_up(s):\n",
    "    new = s.lower()\n",
    "    new1 = re.sub(r'[^a-z]', ' ', new)\n",
    "    return (new1.lstrip().rstrip())\n",
    "def tokenize(s):\n",
    "    words = word_tokenize(s)\n",
    "    return words\n",
    "def remove_stopwords(l):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [w for w in l if not w in stop_words]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# Crime\n",
    "crime['cleaned'] = crime['name'].apply(clean_up)\n",
    "crime['cleaned'] = crime['cleaned'].apply(tokenize)\n",
    "crime['cleaned'] = crime['cleaned'].apply(remove_stopwords)\n",
    "all_crime = []\n",
    "for line in crime.cleaned:\n",
    "    for x in line:\n",
    "        all_crime.append(x)        \n",
    "all_crime1 = nltk.FreqDist(all_crime)\n",
    "all_crime2 = dict(all_crime1)\n",
    "words_crime = dict(sorted(all_crime2.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11dad1290>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Cloud for Crime\n",
    "wordcloud_crime = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_crime)\n",
    "wordcloud_crime.to_file('/Users/victoria/Books/crime.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11db18d90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graphic Novels\n",
    "graph['cleaned'] = graph['name'].apply(clean_up)\n",
    "graph['cleaned'] = graph['cleaned'].apply(tokenize)\n",
    "graph['cleaned'] = graph['cleaned'].apply(remove_stopwords)\n",
    "all_graph = []\n",
    "for line in graph.cleaned:\n",
    "    for x in line:\n",
    "        all_graph.append(x)        \n",
    "all_graph1 = nltk.FreqDist(all_graph)\n",
    "all_graph2 = dict(all_graph1)\n",
    "words_graph = dict(sorted(all_graph2.items(), key=lambda x: x[1], reverse=True))\n",
    "# Word Cloud for Graphic Novels\n",
    "wordcloud_graph = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_graph)\n",
    "wordcloud_graph.to_file('/Users/victoria/Books/graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11dbbf090>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mind Body Spirit\n",
    "mind['cleaned'] = mind['name'].apply(clean_up)\n",
    "mind['cleaned'] = mind['cleaned'].apply(tokenize)\n",
    "mind['cleaned'] = mind['cleaned'].apply(remove_stopwords)\n",
    "all_mind = []\n",
    "for line in mind.cleaned:\n",
    "    for x in line:\n",
    "        all_mind.append(x)        \n",
    "all_mind1 = nltk.FreqDist(all_mind)\n",
    "all_mind2 = dict(all_mind1)\n",
    "words_mind = dict(sorted(all_mind2.items(), key=lambda x: x[1], reverse=True))\n",
    "# Word Cloud for Mind Body Spirit\n",
    "wordcloud_mind = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_mind)\n",
    "wordcloud_mind.to_file('/Users/victoria/Books/mind.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11dc64bd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Romance\n",
    "romance['cleaned'] = romance['name'].apply(clean_up)\n",
    "romance['cleaned'] = romance['cleaned'].apply(tokenize)\n",
    "romance['cleaned'] = romance['cleaned'].apply(remove_stopwords)\n",
    "all_romance = []\n",
    "for line in romance.cleaned:\n",
    "    for x in line:\n",
    "        all_romance.append(x)        \n",
    "all_romance1 = nltk.FreqDist(all_romance)\n",
    "all_romance2 = dict(all_romance1)\n",
    "words_romance = dict(sorted(all_romance2.items(), key=lambda x: x[1], reverse=True))\n",
    "# Word Cloud for Romance\n",
    "wordcloud_romance = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_romance)\n",
    "wordcloud_romance.to_file('/Users/victoria/Books/romance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11dcc1d10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Science\n",
    "science['cleaned'] = science['name'].apply(clean_up)\n",
    "science['cleaned'] = science['cleaned'].apply(tokenize)\n",
    "science['cleaned'] = science['cleaned'].apply(remove_stopwords)\n",
    "all_science = []\n",
    "for line in science.cleaned:\n",
    "    for x in line:\n",
    "        all_science.append(x)        \n",
    "all_science1 = nltk.FreqDist(all_science)\n",
    "all_science2 = dict(all_science1)\n",
    "words_science = dict(sorted(all_science2.items(), key=lambda x: x[1], reverse=True))\n",
    "# Word Cloud for Science\n",
    "wordcloud_science = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_science)\n",
    "wordcloud_science.to_file('/Users/victoria/Books/science.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11dd24cd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Travel\n",
    "travel['cleaned'] = travel['name'].apply(clean_up)\n",
    "travel['cleaned'] = travel['cleaned'].apply(tokenize)\n",
    "travel['cleaned'] = travel['cleaned'].apply(remove_stopwords)\n",
    "all_travel = []\n",
    "for line in travel.cleaned:\n",
    "    for x in line:\n",
    "        all_travel.append(x)        \n",
    "all_travel1 = nltk.FreqDist(all_travel)\n",
    "all_travel2 = dict(all_travel1)\n",
    "words_travel = dict(sorted(all_travel2.items(), key=lambda x: x[1], reverse=True))\n",
    "# Word Cloud for Travel\n",
    "wordcloud_travel = WordCloud(width = 800, height = 800, \n",
    "                            background_color = 'white', stopwords=stopwords, \n",
    "                            min_font_size=10).generate_from_frequencies(words_travel)\n",
    "wordcloud_travel.to_file('/Users/victoria/Books/travel.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "six = pd.read_csv('/Users/victoria/Books/six-genres-to-analyse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "six['category_num'] = np.where(six['category']=='Crime-Thriller', 0, six['category'])\n",
    "six['category_num'] = np.where(six['category']=='Graphic-Novels-Anime-Manga', 1, six['category_num'])\n",
    "six['category_num'] = np.where(six['category']=='Mind-Body-Spirit', 2, six['category_num'])\n",
    "six['category_num'] = np.where(six['category']=='Romance', 3, six['category_num'])\n",
    "six['category_num'] = np.where(six['category']=='Science-Fiction-Fantasy-Horror', 4, six['category_num'])\n",
    "six['category_num'] = np.where(six['category']=='Travel-Holiday-Guides', 5, six['category_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    new = s.lower()\n",
    "    new1 = re.sub(r'[^a-z]', ' ', new)\n",
    "    return (new1.lstrip().rstrip())\n",
    "\n",
    "def tokenize(s):\n",
    "    words = word_tokenize(s)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(l):\n",
    "    stem = [PorterStemmer().stem(w) for w in l]\n",
    "    lem = [WordNetLemmatizer().lemmatize(w) for w in stem]\n",
    "    return lem\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [w for w in l if not w in stop_words]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "six['name_new'] = six['name'].apply(clean_up)\n",
    "six['name_new'] = six['name_new'].apply(tokenize)\n",
    "six['name_new'] = six['name_new'].apply(stem_and_lemmatize)\n",
    "six['name_new'] = six['name_new'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>book_depository_stars</th>\n",
       "      <th>price</th>\n",
       "      <th>isbn</th>\n",
       "      <th>category</th>\n",
       "      <th>img_paths</th>\n",
       "      <th>category_num</th>\n",
       "      <th>name_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7900</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>1984</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>9780141036144</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "      <td>dataset/Crime-Thriller/0000001.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7901</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Man's Search For Meaning</td>\n",
       "      <td>Viktor E. Frankl</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9781846041242</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "      <td>dataset/Crime-Thriller/0000002.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[man, search, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7902</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.98</td>\n",
       "      <td>9780141036137</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "      <td>dataset/Crime-Thriller/0000003.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[anim, farm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7903</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>The Husband's Secret</td>\n",
       "      <td>Liane Moriarty</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9781405911665</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "      <td>dataset/Crime-Thriller/0000004.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[husband, secret]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7904</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Gone Girl</td>\n",
       "      <td>Gillian Flynn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.55</td>\n",
       "      <td>9781780221359</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "      <td>dataset/Crime-Thriller/0000005.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[gone, girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>32576</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Elementary Korean Workbook</td>\n",
       "      <td>Insun Lee</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.52</td>\n",
       "      <td>9780804845021</td>\n",
       "      <td>Travel-Holiday-Guides</td>\n",
       "      <td>dataset/Travel-Holiday-Guides/0000985.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>[elementari, korean, workbook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>32577</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Lonely Planet Best of Peru</td>\n",
       "      <td>Lonely Planet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.26</td>\n",
       "      <td>9781786571267</td>\n",
       "      <td>Travel-Holiday-Guides</td>\n",
       "      <td>dataset/Travel-Holiday-Guides/0000986.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>[lone, planet, best, peru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>32578</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Complete Finnish Beginner to Intermediate Cour...</td>\n",
       "      <td>Terttu Leney</td>\n",
       "      <td>4.5</td>\n",
       "      <td>43.12</td>\n",
       "      <td>9781444195224</td>\n",
       "      <td>Travel-Holiday-Guides</td>\n",
       "      <td>dataset/Travel-Holiday-Guides/0000987.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>[complet, finnish, beginn, intermedi, cours, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>32579</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>Simple Thai Food</td>\n",
       "      <td>Leela Punyaratabandhu</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>9781607745235</td>\n",
       "      <td>Travel-Holiday-Guides</td>\n",
       "      <td>dataset/Travel-Holiday-Guides/0000988.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>[simpl, thai, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>32580</td>\n",
       "      <td>https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...</td>\n",
       "      <td>L'Appart</td>\n",
       "      <td>David Lebovitz</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.40</td>\n",
       "      <td>9780804188388</td>\n",
       "      <td>Travel-Holiday-Guides</td>\n",
       "      <td>dataset/Travel-Holiday-Guides/0000989.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>[l, appart]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5921 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              image  \\\n",
       "0           7900  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "1           7901  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "2           7902  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "3           7903  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "4           7904  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "...          ...                                                ...   \n",
       "5916       32576  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "5917       32577  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "5918       32578  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "5919       32579  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "5920       32580  https://d1w7fb2mkkr3kw.cloudfront.net/assets/i...   \n",
       "\n",
       "                                                   name  \\\n",
       "0                                                  1984   \n",
       "1                              Man's Search For Meaning   \n",
       "2                                           Animal Farm   \n",
       "3                                  The Husband's Secret   \n",
       "4                                             Gone Girl   \n",
       "...                                                 ...   \n",
       "5916                         Elementary Korean Workbook   \n",
       "5917                         Lonely Planet Best of Peru   \n",
       "5918  Complete Finnish Beginner to Intermediate Cour...   \n",
       "5919                                   Simple Thai Food   \n",
       "5920                                           L'Appart   \n",
       "\n",
       "                     author  book_depository_stars  price           isbn  \\\n",
       "0             George Orwell                    4.0   7.88  9780141036144   \n",
       "1          Viktor E. Frankl                    4.5   9.66  9781846041242   \n",
       "2             George Orwell                    4.0   7.98  9780141036137   \n",
       "3            Liane Moriarty                    4.0   9.33  9781405911665   \n",
       "4             Gillian Flynn                    4.0   9.55  9781780221359   \n",
       "...                     ...                    ...    ...            ...   \n",
       "5916              Insun Lee                    4.0  15.52  9780804845021   \n",
       "5917          Lonely Planet                    4.0  14.26  9781786571267   \n",
       "5918           Terttu Leney                    4.5  43.12  9781444195224   \n",
       "5919  Leela Punyaratabandhu                    4.0  20.84  9781607745235   \n",
       "5920         David Lebovitz                    3.5  21.40  9780804188388   \n",
       "\n",
       "                   category                                  img_paths  \\\n",
       "0            Crime-Thriller         dataset/Crime-Thriller/0000001.jpg   \n",
       "1            Crime-Thriller         dataset/Crime-Thriller/0000002.jpg   \n",
       "2            Crime-Thriller         dataset/Crime-Thriller/0000003.jpg   \n",
       "3            Crime-Thriller         dataset/Crime-Thriller/0000004.jpg   \n",
       "4            Crime-Thriller         dataset/Crime-Thriller/0000005.jpg   \n",
       "...                     ...                                        ...   \n",
       "5916  Travel-Holiday-Guides  dataset/Travel-Holiday-Guides/0000985.jpg   \n",
       "5917  Travel-Holiday-Guides  dataset/Travel-Holiday-Guides/0000986.jpg   \n",
       "5918  Travel-Holiday-Guides  dataset/Travel-Holiday-Guides/0000987.jpg   \n",
       "5919  Travel-Holiday-Guides  dataset/Travel-Holiday-Guides/0000988.jpg   \n",
       "5920  Travel-Holiday-Guides  dataset/Travel-Holiday-Guides/0000989.jpg   \n",
       "\n",
       "     category_num                                           name_new  \n",
       "0               0                                                 []  \n",
       "1               0                                [man, search, mean]  \n",
       "2               0                                       [anim, farm]  \n",
       "3               0                                  [husband, secret]  \n",
       "4               0                                       [gone, girl]  \n",
       "...           ...                                                ...  \n",
       "5916            5                     [elementari, korean, workbook]  \n",
       "5917            5                         [lone, planet, best, peru]  \n",
       "5918            5  [complet, finnish, beginn, intermedi, cours, l...  \n",
       "5919            5                                [simpl, thai, food]  \n",
       "5920            5                                        [l, appart]  \n",
       "\n",
       "[5921 rows x 11 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for line in six.name_new:\n",
    "    for x in line:\n",
    "        all_words.append(x)\n",
    "        \n",
    "all_words1 = nltk.FreqDist(all_words)\n",
    "all_words2 = dict(all_words1)\n",
    "words = dict(sorted(all_words2.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3701"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = list(words.keys())\n",
    "len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vol',\n",
       " 'planet',\n",
       " 'lone',\n",
       " 'volum',\n",
       " 'book',\n",
       " 'edit',\n",
       " 'one',\n",
       " 'death',\n",
       " 'walk',\n",
       " 'light',\n",
       " 'dead',\n",
       " 'man',\n",
       " 'world',\n",
       " 'dark',\n",
       " 'life',\n",
       " 'novel',\n",
       " 'guid',\n",
       " 'love',\n",
       " 'new',\n",
       " 'card',\n",
       " 'secret',\n",
       " 'tarot',\n",
       " 'girl',\n",
       " 'oracl',\n",
       " 'mind',\n",
       " 'asterix',\n",
       " 'travel',\n",
       " 'magic',\n",
       " 'last',\n",
       " 'night',\n",
       " 'complet',\n",
       " 'war',\n",
       " 'part',\n",
       " 'box',\n",
       " 'citi',\n",
       " 'set',\n",
       " 'berserk',\n",
       " 'harri',\n",
       " 'game',\n",
       " 'black',\n",
       " 'adventur',\n",
       " 'potter',\n",
       " 'littl',\n",
       " 'blood',\n",
       " 'way',\n",
       " 'angel',\n",
       " 'pocket',\n",
       " 'tokyo',\n",
       " 'time',\n",
       " 'fire',\n",
       " 'art',\n",
       " 'hero',\n",
       " 'hous',\n",
       " 'stori',\n",
       " 'best',\n",
       " 'power',\n",
       " 'saga',\n",
       " 'map',\n",
       " 'blue',\n",
       " 'batman',\n",
       " 'heal',\n",
       " 'moon',\n",
       " 'live',\n",
       " 'lover',\n",
       " 'road',\n",
       " 'titan',\n",
       " 'yoga',\n",
       " 'good',\n",
       " 'bodi',\n",
       " 'beauti',\n",
       " 'day',\n",
       " 'attack',\n",
       " 'pari',\n",
       " 'dragon',\n",
       " 'thi',\n",
       " 'lost',\n",
       " 'shadow',\n",
       " 'happi',\n",
       " 'promis',\n",
       " 'ghoul',\n",
       " 'atla',\n",
       " 'red',\n",
       " 'heart',\n",
       " 'archangel',\n",
       " 'pilgrim',\n",
       " 'star',\n",
       " 'dream',\n",
       " 'york',\n",
       " 'note',\n",
       " 'king',\n",
       " 'four',\n",
       " 'london',\n",
       " 'soul',\n",
       " 'god',\n",
       " 'wild',\n",
       " 'woman',\n",
       " 'witch',\n",
       " 'three',\n",
       " 'thing',\n",
       " 'histori',\n",
       " 'trilog',\n",
       " 'omnibu',\n",
       " 'deck',\n",
       " 'anim',\n",
       " 'midnight',\n",
       " 'tale',\n",
       " 'two',\n",
       " 'v',\n",
       " 'lie',\n",
       " 'seven',\n",
       " 'rise',\n",
       " 'home',\n",
       " 'go',\n",
       " 'bone',\n",
       " 'child',\n",
       " 'assassin',\n",
       " 'year',\n",
       " 'wolf',\n",
       " 'academia',\n",
       " 'piec',\n",
       " 'sword',\n",
       " 'place',\n",
       " 'lord',\n",
       " 'spice',\n",
       " 'hunter',\n",
       " 'gone',\n",
       " 'delux',\n",
       " 'avatar',\n",
       " 'airbend',\n",
       " 'wisdom',\n",
       " 'dk',\n",
       " 'eyewit',\n",
       " 'sister',\n",
       " 'kill',\n",
       " 'tower',\n",
       " 'earth',\n",
       " 'summer',\n",
       " 'sleep',\n",
       " 'law',\n",
       " 'journey',\n",
       " 'seri',\n",
       " 'univers',\n",
       " 'spirit',\n",
       " 'scott',\n",
       " 'dictionari',\n",
       " 'end',\n",
       " 'doctor',\n",
       " 'classic',\n",
       " 'kiss',\n",
       " 'colour',\n",
       " 'punch',\n",
       " 'eat',\n",
       " 'island',\n",
       " 'murder',\n",
       " 'stranger',\n",
       " 'ladi',\n",
       " 'die',\n",
       " 'famili',\n",
       " 'crystal',\n",
       " 'storm',\n",
       " 'medit',\n",
       " 'essenti',\n",
       " 'big',\n",
       " 'shade',\n",
       " 'castl',\n",
       " 'burn',\n",
       " 'ice',\n",
       " 'run',\n",
       " 'miracl',\n",
       " 'befor',\n",
       " 'wait',\n",
       " 'x',\n",
       " 'call',\n",
       " 'great',\n",
       " 'white',\n",
       " 'mr',\n",
       " 'long',\n",
       " 'fall',\n",
       " 'devil',\n",
       " 'back',\n",
       " 'heaven',\n",
       " 'princ',\n",
       " 'throne',\n",
       " 'flower',\n",
       " 'stone',\n",
       " 'street',\n",
       " 'grave',\n",
       " 'diari',\n",
       " 'jojo',\n",
       " 'bizarr',\n",
       " 'wick',\n",
       " 'medicin',\n",
       " 'japan',\n",
       " 'u',\n",
       " 'six',\n",
       " 'fifti',\n",
       " 'perfect',\n",
       " 'ten',\n",
       " 'let',\n",
       " 'silent',\n",
       " 'origin',\n",
       " 'side',\n",
       " 'american',\n",
       " 'old',\n",
       " 'five',\n",
       " 'wife',\n",
       " 'awaken',\n",
       " 'cat',\n",
       " 'fear',\n",
       " 'first',\n",
       " 'naruto',\n",
       " 'self',\n",
       " 'french',\n",
       " 'dog',\n",
       " 'pictur',\n",
       " 'gift',\n",
       " 'name',\n",
       " 'rose',\n",
       " 'snow',\n",
       " 'peopl',\n",
       " 'garden',\n",
       " 'monster',\n",
       " 'th',\n",
       " 'chang',\n",
       " 'bibl',\n",
       " 'collect',\n",
       " 'fairi',\n",
       " 'queen',\n",
       " 'born',\n",
       " 'plan',\n",
       " 'legend',\n",
       " 'wonderland',\n",
       " 'sea',\n",
       " 'rider',\n",
       " 'cours',\n",
       " 'oil',\n",
       " 'trip',\n",
       " 'phrasebook',\n",
       " 'crow',\n",
       " 'collector',\n",
       " 'door',\n",
       " 'sky',\n",
       " 'strang',\n",
       " 'miss',\n",
       " 'whisper',\n",
       " 'land',\n",
       " 'tree',\n",
       " 'men',\n",
       " 'tell',\n",
       " 'warrior',\n",
       " 'sacr',\n",
       " 'librari',\n",
       " 'chronicl',\n",
       " 'empir',\n",
       " 'boy',\n",
       " 'twenti',\n",
       " 'voic',\n",
       " 'danc',\n",
       " 'path',\n",
       " 'neverland',\n",
       " 'hobbit',\n",
       " 'work',\n",
       " 'ball',\n",
       " 'bride',\n",
       " 'frankenstein',\n",
       " 'pray',\n",
       " 'camino',\n",
       " 'beginn',\n",
       " 'vintag',\n",
       " 'europ',\n",
       " 'search',\n",
       " 'farm',\n",
       " 'behind',\n",
       " 'dorian',\n",
       " 'gray',\n",
       " 'natur',\n",
       " 'next',\n",
       " 'eye',\n",
       " 'still',\n",
       " 'hour',\n",
       " 'silenc',\n",
       " 'true',\n",
       " 'keeper',\n",
       " 'hidden',\n",
       " 'de',\n",
       " 'daughter',\n",
       " 'keep',\n",
       " 'bird',\n",
       " 'beach',\n",
       " 'play',\n",
       " 'touch',\n",
       " 'kingdom',\n",
       " 'winter',\n",
       " 'sweet',\n",
       " 'dr',\n",
       " 'italian',\n",
       " 'someth',\n",
       " 'christma',\n",
       " 'tini',\n",
       " 'chamber',\n",
       " 'graphic',\n",
       " 'wind',\n",
       " 'languag',\n",
       " 'edg',\n",
       " 'outland',\n",
       " 'south',\n",
       " 'ring',\n",
       " 'foundat',\n",
       " 'itali',\n",
       " 'australia',\n",
       " 'see',\n",
       " 'high',\n",
       " 'evil',\n",
       " 'never',\n",
       " 'second',\n",
       " 'park',\n",
       " 'get',\n",
       " 'bound',\n",
       " 'glass',\n",
       " 'mysteri',\n",
       " 'countri',\n",
       " 'greatest',\n",
       " 'alway',\n",
       " 'memori',\n",
       " 'beast',\n",
       " 'like',\n",
       " 'john',\n",
       " 'make',\n",
       " 'letter',\n",
       " 'code',\n",
       " 'cross',\n",
       " 'haunt',\n",
       " 'ancient',\n",
       " 'onc',\n",
       " 'count',\n",
       " 'look',\n",
       " 'dracula',\n",
       " 'marvel',\n",
       " 'paper',\n",
       " 'butler',\n",
       " 'vagabond',\n",
       " 'divin',\n",
       " 'preacher',\n",
       " 'east',\n",
       " 'fabl',\n",
       " 'green',\n",
       " 'yotsuba',\n",
       " 'food',\n",
       " 'far',\n",
       " 'word',\n",
       " 'read',\n",
       " 'therapi',\n",
       " 'illustr',\n",
       " 'darkest',\n",
       " 'dune',\n",
       " 'zealand',\n",
       " 'player',\n",
       " 'room',\n",
       " 'train',\n",
       " 'close',\n",
       " 'alic',\n",
       " 'darker',\n",
       " 'river',\n",
       " 'cold',\n",
       " 'orang',\n",
       " 'tri',\n",
       " 'spider',\n",
       " 'son',\n",
       " 'broken',\n",
       " 'air',\n",
       " 'wrong',\n",
       " 'morn',\n",
       " 'z',\n",
       " 'modern',\n",
       " 'water',\n",
       " 'wood',\n",
       " 'hand',\n",
       " 'onli',\n",
       " 'ever',\n",
       " 'season',\n",
       " 'sun',\n",
       " 'come',\n",
       " 'vision',\n",
       " 'penguin',\n",
       " 'highland',\n",
       " 'vader',\n",
       " 'overlord',\n",
       " 'vizbig',\n",
       " 'knight',\n",
       " 'tintin',\n",
       " 'exorcist',\n",
       " 'ghost',\n",
       " 'hellboy',\n",
       " 'made',\n",
       " 'find',\n",
       " 'pluto',\n",
       " 'medic',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'step',\n",
       " 'journal',\n",
       " 'health',\n",
       " 'rich',\n",
       " 'handbook',\n",
       " 'autumn',\n",
       " 'song',\n",
       " 'top',\n",
       " 'coast',\n",
       " 'hi',\n",
       " 'nine',\n",
       " 'clockwork',\n",
       " 'without',\n",
       " 'martian',\n",
       " 'anniversari',\n",
       " 'killer',\n",
       " 'ii',\n",
       " 'justic',\n",
       " 'blind',\n",
       " 'wed',\n",
       " 'west',\n",
       " 'tea',\n",
       " 'break',\n",
       " 'sherlock',\n",
       " 'holm',\n",
       " 'right',\n",
       " 'noth',\n",
       " 'rule',\n",
       " 'bali',\n",
       " 'return',\n",
       " 'deadli',\n",
       " 'dawn',\n",
       " 'lake',\n",
       " 'immort',\n",
       " 'tiger',\n",
       " 'extraordinari',\n",
       " 'pleasur',\n",
       " 'philosoph',\n",
       " 'cookbook',\n",
       " 'darth',\n",
       " 'kid',\n",
       " 'encyclopedia',\n",
       " 'stilton',\n",
       " 'sandman',\n",
       " 'classroom',\n",
       " 'goodnight',\n",
       " 'punpun',\n",
       " 'practic',\n",
       " 'color',\n",
       " 'super',\n",
       " 'spain',\n",
       " 'abyss',\n",
       " 'machin',\n",
       " 'anoth',\n",
       " 'wake',\n",
       " 'ultim',\n",
       " 'lucki',\n",
       " 'care',\n",
       " 'medium',\n",
       " 'salt',\n",
       " 'goddess',\n",
       " 'tao',\n",
       " 'inner',\n",
       " 'relax',\n",
       " 'workbook',\n",
       " 'fifth',\n",
       " 'kitchen',\n",
       " 'thorn',\n",
       " 'hill',\n",
       " 'voyag',\n",
       " 'austen',\n",
       " 'ship',\n",
       " 'gate',\n",
       " 'fool',\n",
       " 'bike',\n",
       " 'vietnam',\n",
       " 'railway',\n",
       " 'rome',\n",
       " 'scotland',\n",
       " 'trail',\n",
       " 'africa',\n",
       " 'husband',\n",
       " 'readi',\n",
       " 'grey',\n",
       " 'evelyn',\n",
       " 'shine',\n",
       " 'everyth',\n",
       " 'golden',\n",
       " 'ground',\n",
       " 'column',\n",
       " 'psycho',\n",
       " 'matter',\n",
       " 'mar',\n",
       " 'thousand',\n",
       " 'truth',\n",
       " 'ocean',\n",
       " 'case',\n",
       " 'reckon',\n",
       " 'academi',\n",
       " 'gold',\n",
       " 'type',\n",
       " 'full',\n",
       " 'wa',\n",
       " 'person',\n",
       " 'crusad',\n",
       " 'breath',\n",
       " 'berlin',\n",
       " 'hors',\n",
       " 'phantom',\n",
       " 'mother',\n",
       " 'watch',\n",
       " 'agatha',\n",
       " 'raisin',\n",
       " 'wonder',\n",
       " 'pop',\n",
       " 'hope',\n",
       " 'zero',\n",
       " 'hunt',\n",
       " 'russian',\n",
       " 'final',\n",
       " 'peac',\n",
       " 'mile',\n",
       " 'compendium',\n",
       " 'whi',\n",
       " 'deadpool',\n",
       " 'babi',\n",
       " 'monstress',\n",
       " 'princess',\n",
       " 'akira',\n",
       " 'tail',\n",
       " 'vinland',\n",
       " 'sailor',\n",
       " 'sex',\n",
       " 'fullmet',\n",
       " 'alchemist',\n",
       " 'need',\n",
       " 'deadman',\n",
       " 'fell',\n",
       " 'pick',\n",
       " 'dungeon',\n",
       " 'mortal',\n",
       " 'instrument',\n",
       " 'zen',\n",
       " 'sunston',\n",
       " 'realli',\n",
       " 'le',\n",
       " 'r',\n",
       " 'delirium',\n",
       " 'ride',\n",
       " 'rescu',\n",
       " 'habit',\n",
       " 'te',\n",
       " 'wheel',\n",
       " 'magician',\n",
       " 'destini',\n",
       " 'chakra',\n",
       " 'messag',\n",
       " 'grow',\n",
       " 'teach',\n",
       " 'reissu',\n",
       " 'human',\n",
       " 'australian',\n",
       " 'north',\n",
       " 'herbal',\n",
       " 'music',\n",
       " 'rosi',\n",
       " 'pride',\n",
       " 'prejudic',\n",
       " 'mine',\n",
       " 'etern',\n",
       " 'posse',\n",
       " 'motorcycl',\n",
       " 'forest',\n",
       " 'omen',\n",
       " 'hitchhik',\n",
       " 'galaxi',\n",
       " 'ireland',\n",
       " 'tour',\n",
       " 'bad',\n",
       " 'rite',\n",
       " 'tattoo',\n",
       " 'echo',\n",
       " 'club',\n",
       " 'hotel',\n",
       " 'hard',\n",
       " 'draw',\n",
       " 'affair',\n",
       " 'hang',\n",
       " 'spi',\n",
       " 'outsid',\n",
       " 'dexter',\n",
       " 'leopard',\n",
       " 'station',\n",
       " 'tomorrow',\n",
       " 'english',\n",
       " 'iron',\n",
       " 'brotherhood',\n",
       " 'fli',\n",
       " 'school',\n",
       " 'number',\n",
       " 'shore',\n",
       " 'st',\n",
       " 'royal',\n",
       " 'sign',\n",
       " 'sin',\n",
       " 'trap',\n",
       " 'nd',\n",
       " 'quiet',\n",
       " 'bar',\n",
       " 'dust',\n",
       " 'march',\n",
       " 'demon',\n",
       " 'seduct',\n",
       " 'simpl',\n",
       " 'jack',\n",
       " 'chain',\n",
       " 'billi',\n",
       " 'master',\n",
       " 'merci',\n",
       " 'battl',\n",
       " 'jekyl',\n",
       " 'hyde',\n",
       " 'real',\n",
       " 'fate',\n",
       " 'talk',\n",
       " 'greek',\n",
       " 'guardian',\n",
       " 'well',\n",
       " 'say',\n",
       " 'prison',\n",
       " 'azkaban',\n",
       " 'persepoli',\n",
       " 'dear',\n",
       " 'zone',\n",
       " 'amaz',\n",
       " 'manga',\n",
       " 'face',\n",
       " 'britain',\n",
       " 'thea',\n",
       " 'blade',\n",
       " 'move',\n",
       " 'haikyu',\n",
       " 'onlin',\n",
       " 'everi',\n",
       " 'half',\n",
       " 'stardust',\n",
       " 'seed',\n",
       " 'destruct',\n",
       " 'propheci',\n",
       " 'becom',\n",
       " 'easi',\n",
       " 'mountain',\n",
       " 'venic',\n",
       " 'effect',\n",
       " 'diamond',\n",
       " 'trauma',\n",
       " 'solo',\n",
       " 'smoke',\n",
       " 'forev',\n",
       " 'agreement',\n",
       " 'liver',\n",
       " 'daili',\n",
       " 'ching',\n",
       " 'monk',\n",
       " 'rais',\n",
       " 'eastern',\n",
       " 'thought',\n",
       " 'revis',\n",
       " 'anatomi',\n",
       " 'surrend',\n",
       " 'mani',\n",
       " 'energi',\n",
       " 'emot',\n",
       " 'spiritu',\n",
       " 'santiago',\n",
       " 'radianc',\n",
       " 'mini',\n",
       " 'may',\n",
       " 'thoth',\n",
       " 'plant',\n",
       " 'hygg',\n",
       " 'know',\n",
       " 'free',\n",
       " 'wise',\n",
       " 'reveal',\n",
       " 'anna',\n",
       " 'alon',\n",
       " 'quest',\n",
       " 'cook',\n",
       " 'japanes',\n",
       " 'ash',\n",
       " 'around',\n",
       " 'rememb',\n",
       " 'mayb',\n",
       " 'bite',\n",
       " 'destin',\n",
       " 'deep',\n",
       " 'crown',\n",
       " 'india',\n",
       " 'fantast',\n",
       " 'feast',\n",
       " 'small',\n",
       " 'silmarillion',\n",
       " 'reaper',\n",
       " 'abroad',\n",
       " 'holi',\n",
       " 'rick',\n",
       " 'provenc',\n",
       " 'singapor',\n",
       " 'usa',\n",
       " 'intermedi',\n",
       " 'phrase',\n",
       " 'mean',\n",
       " 'bloodi',\n",
       " 'nineteen',\n",
       " 'hardcastl',\n",
       " 'express',\n",
       " 'sing',\n",
       " 'forc',\n",
       " 'past',\n",
       " 'shrug',\n",
       " 'viciou',\n",
       " 'gather',\n",
       " 'pillar',\n",
       " 'window',\n",
       " 'stand',\n",
       " 'incred',\n",
       " 'line',\n",
       " 'discoveri',\n",
       " 'ministri',\n",
       " 'utmost',\n",
       " 'ancillari',\n",
       " 'jurass',\n",
       " 'foxglov',\n",
       " 'twice',\n",
       " 'pet',\n",
       " 'sematari',\n",
       " 'drive',\n",
       " 'iii',\n",
       " 'grace',\n",
       " 'darkli',\n",
       " 'worth',\n",
       " 'tear',\n",
       " 'trick',\n",
       " 'nake',\n",
       " 'heat',\n",
       " 'preciou',\n",
       " 'lamb',\n",
       " 'start',\n",
       " 'lose',\n",
       " 'want',\n",
       " 'crave',\n",
       " 'turn',\n",
       " 'slow',\n",
       " 'invis',\n",
       " 'vienna',\n",
       " 'father',\n",
       " 'clean',\n",
       " 'clockmak',\n",
       " 'mist',\n",
       " 'short',\n",
       " 'ob',\n",
       " 'giant',\n",
       " 'holiday',\n",
       " 'innoc',\n",
       " 'legendari',\n",
       " 'daisi',\n",
       " 'raptur',\n",
       " 'hannib',\n",
       " 'eagl',\n",
       " 'wrath',\n",
       " 'companion',\n",
       " 'aliv',\n",
       " 'vendetta',\n",
       " 'fortun',\n",
       " 'beneath',\n",
       " 'tooth',\n",
       " 'reunion',\n",
       " 'shift',\n",
       " 'butterfli',\n",
       " 'crimin',\n",
       " 'wish',\n",
       " 'surpris',\n",
       " 'away',\n",
       " 'togeth',\n",
       " 'southern',\n",
       " 'vampir',\n",
       " 'bear',\n",
       " 'gentleman',\n",
       " 'mark',\n",
       " 'hair',\n",
       " 'happen',\n",
       " 'guilti',\n",
       " 'survivor',\n",
       " 'fallen',\n",
       " 'hufflepuff',\n",
       " 'goblet',\n",
       " 'ugli',\n",
       " 'enchant',\n",
       " 'experi',\n",
       " 'warcraft',\n",
       " 'answer',\n",
       " 'normal',\n",
       " 'weird',\n",
       " 'rat',\n",
       " 'pokemon',\n",
       " 'leagu',\n",
       " 'neon',\n",
       " 'genesi',\n",
       " 'evangelion',\n",
       " 'sorrow',\n",
       " 'build',\n",
       " 'shield',\n",
       " 'tin',\n",
       " 'bitch',\n",
       " 'calm',\n",
       " 'explor',\n",
       " 'remain',\n",
       " 'mermaid',\n",
       " 'progress',\n",
       " 'bed',\n",
       " 'shadowhunt',\n",
       " 'foot',\n",
       " 'bloom',\n",
       " 'electr',\n",
       " 'order',\n",
       " 'urasawa',\n",
       " 'tezuka',\n",
       " 'rift',\n",
       " 'america',\n",
       " 'lock',\n",
       " 'host',\n",
       " 'pack',\n",
       " 'offici',\n",
       " 'p',\n",
       " 'milk',\n",
       " 'honey',\n",
       " 'proof',\n",
       " 'ha',\n",
       " 'convers',\n",
       " 'western',\n",
       " 'sutra',\n",
       " 'aromatherapi',\n",
       " 'attract',\n",
       " 'posit',\n",
       " 'enneagram',\n",
       " 'herb',\n",
       " 'ayurveda',\n",
       " 'brief',\n",
       " 'process',\n",
       " 'lenormand',\n",
       " 'pharmaci',\n",
       " 'awar',\n",
       " 'witchcraft',\n",
       " 'subconsci',\n",
       " 'pain',\n",
       " 'freedom',\n",
       " 'ink',\n",
       " 'solut',\n",
       " 'money',\n",
       " 'portugu',\n",
       " 'stress',\n",
       " 'spell',\n",
       " 'intuit',\n",
       " 'sen',\n",
       " 'beyond',\n",
       " 'passion',\n",
       " 'materi',\n",
       " 'karenina',\n",
       " 'jone',\n",
       " 'amber',\n",
       " 'china',\n",
       " 'left',\n",
       " 'wuther',\n",
       " 'height',\n",
       " 'poldark',\n",
       " 'drum',\n",
       " 'shop',\n",
       " 'someday',\n",
       " 'flight',\n",
       " 'silver',\n",
       " 'lane',\n",
       " 'circu',\n",
       " 'steel',\n",
       " 'infern',\n",
       " 'devic',\n",
       " 'ender',\n",
       " 'guard',\n",
       " 'republ',\n",
       " 'sri',\n",
       " 'lanka',\n",
       " 'epic',\n",
       " 'portug',\n",
       " 'shoestr',\n",
       " 'iceland',\n",
       " 'northern',\n",
       " 'wine',\n",
       " 'sicili',\n",
       " 'indonesia',\n",
       " 'kyoto',\n",
       " 'franc',\n",
       " 'audio',\n",
       " 'san',\n",
       " 'wellington',\n",
       " 'napoleon',\n",
       " 'curiou',\n",
       " 'friend',\n",
       " 'thirteen',\n",
       " 'eighti',\n",
       " 'burial',\n",
       " 'orient',\n",
       " 'project',\n",
       " 'detect',\n",
       " 'coupl',\n",
       " 'caus',\n",
       " 'told',\n",
       " 'loud',\n",
       " 'safari',\n",
       " 'take',\n",
       " 'serial',\n",
       " 'dispossess',\n",
       " 'salem',\n",
       " 'lot',\n",
       " 'grip',\n",
       " 'bestsel',\n",
       " 'author',\n",
       " 'bat',\n",
       " 'passag',\n",
       " 'saturday',\n",
       " 'parti',\n",
       " 'knife',\n",
       " 'carri',\n",
       " 'barrist',\n",
       " 'nemesi',\n",
       " 'confess',\n",
       " 'sexi',\n",
       " 'tradit',\n",
       " 'inferno',\n",
       " 'vii',\n",
       " 'hyperion',\n",
       " 'mouse',\n",
       " 'kind',\n",
       " 'wizard',\n",
       " 'hell',\n",
       " 'octob',\n",
       " 'intent',\n",
       " 'crime',\n",
       " 'punish',\n",
       " 'twin',\n",
       " 'peak',\n",
       " 'picnic',\n",
       " 'earli',\n",
       " 'legaci',\n",
       " 'privat',\n",
       " 'bring',\n",
       " 'cafe',\n",
       " 'brother',\n",
       " 'raven',\n",
       " 'wave',\n",
       " 'mous',\n",
       " 'twelv',\n",
       " 'plum',\n",
       " 'twist',\n",
       " 'munich',\n",
       " 'chalk',\n",
       " 'hopeless',\n",
       " 'suspect',\n",
       " 'brown',\n",
       " 'tie',\n",
       " 'scarlet',\n",
       " 'factor',\n",
       " 'thiev',\n",
       " 'circl',\n",
       " 'revel',\n",
       " 'pig',\n",
       " 'flame',\n",
       " 'moscow',\n",
       " 'wit',\n",
       " 'appl',\n",
       " 'valley',\n",
       " 'unlik',\n",
       " 'rd',\n",
       " 'mouth',\n",
       " 'conquest',\n",
       " 'mask',\n",
       " 'finger',\n",
       " 'fifteen',\n",
       " 'across',\n",
       " 'fight',\n",
       " 'noir',\n",
       " 'bye',\n",
       " 'pattern',\n",
       " 'third',\n",
       " 'among',\n",
       " 'bell',\n",
       " 'falcon',\n",
       " 'sugar',\n",
       " 'whole',\n",
       " 'furi',\n",
       " 'horseman',\n",
       " 'bk',\n",
       " ...]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(s):\n",
    "    score_list=[]\n",
    "    for x in words_list:\n",
    "        if x in s:\n",
    "            score_list.append(1)\n",
    "        else:\n",
    "            score_list.append(0)\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "six['name_score'] = six['name_new'].apply(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_for_model = six[['Unnamed: 0','name', 'author', 'name_new', 'name_score', 'category_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_for_model.to_json('/Users/victoria/Books/titles_for_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_json('/Users/victoria/Books/titles_for_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_shuffled = titles.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>name_new</th>\n",
       "      <th>name_score</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11961</td>\n",
       "      <td>My Hero Academia, Vol. 19</td>\n",
       "      <td>Kohei Horikoshi</td>\n",
       "      <td>[hero, academia, vol]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17755</td>\n",
       "      <td>Tarot of Pagan Cats</td>\n",
       "      <td>Lo Scarabeo</td>\n",
       "      <td>[tarot, pagan, cat]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11919</td>\n",
       "      <td>The Walking Dead Compendium Volume 2</td>\n",
       "      <td>Robert Kirkman</td>\n",
       "      <td>[walk, dead, compendium, volum]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8723</td>\n",
       "      <td>Heat Rises</td>\n",
       "      <td>Richard Castle</td>\n",
       "      <td>[heat, rise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12648</td>\n",
       "      <td>The Walking Dead Volume 25: No Turning Back</td>\n",
       "      <td>Robert Kirkman</td>\n",
       "      <td>[walk, dead, volum, turn, back]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>24115</td>\n",
       "      <td>Elantris</td>\n",
       "      <td>Brandon Sanderson</td>\n",
       "      <td>[elantri]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>8501</td>\n",
       "      <td>Imperium</td>\n",
       "      <td>Robert Harris</td>\n",
       "      <td>[imperium]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>12560</td>\n",
       "      <td>Dragon Ball (3-in-1 Edition), Vol. 3</td>\n",
       "      <td>Akira Toriyama</td>\n",
       "      <td>[dragon, ball, edit, vol]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>23995</td>\n",
       "      <td>Shadow of Night</td>\n",
       "      <td>Deborah Harkness</td>\n",
       "      <td>[shadow, night]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>24637</td>\n",
       "      <td>Archangel's Kiss</td>\n",
       "      <td>Nalini Singh</td>\n",
       "      <td>[archangel, kiss]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5921 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                         name  \\\n",
       "0          11961                    My Hero Academia, Vol. 19   \n",
       "1          17755                          Tarot of Pagan Cats   \n",
       "2          11919         The Walking Dead Compendium Volume 2   \n",
       "3           8723                                   Heat Rises   \n",
       "4          12648  The Walking Dead Volume 25: No Turning Back   \n",
       "...          ...                                          ...   \n",
       "5916       24115                                     Elantris   \n",
       "5917        8501                                     Imperium   \n",
       "5918       12560         Dragon Ball (3-in-1 Edition), Vol. 3   \n",
       "5919       23995                              Shadow of Night   \n",
       "5920       24637                             Archangel's Kiss   \n",
       "\n",
       "                 author                         name_new  \\\n",
       "0       Kohei Horikoshi            [hero, academia, vol]   \n",
       "1           Lo Scarabeo              [tarot, pagan, cat]   \n",
       "2        Robert Kirkman  [walk, dead, compendium, volum]   \n",
       "3        Richard Castle                     [heat, rise]   \n",
       "4        Robert Kirkman  [walk, dead, volum, turn, back]   \n",
       "...                 ...                              ...   \n",
       "5916  Brandon Sanderson                        [elantri]   \n",
       "5917      Robert Harris                       [imperium]   \n",
       "5918     Akira Toriyama        [dragon, ball, edit, vol]   \n",
       "5919   Deborah Harkness                  [shadow, night]   \n",
       "5920       Nalini Singh                [archangel, kiss]   \n",
       "\n",
       "                                             name_score  category_num  \n",
       "0     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             1  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             2  \n",
       "2     [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...             1  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0  \n",
       "4     [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...             1  \n",
       "...                                                 ...           ...  \n",
       "5916  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             4  \n",
       "5917  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0  \n",
       "5918  [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             1  \n",
       "5919  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             4  \n",
       "5920  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             4  \n",
       "\n",
       "[5921 rows x 6 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titles_shuffled.name_score\n",
    "y = titles_shuffled.category_num\n",
    "X_train = np.vstack(X[:4736])\n",
    "X_test = np.vstack(X[4736:])\n",
    "y_train = y[:4736]\n",
    "y_test = y[4736:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 1.5053 - accuracy: 0.4331\n",
      "Epoch 2/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.8676 - accuracy: 0.7035\n",
      "Epoch 3/15\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4867 - accuracy: 0.8298\n",
      "Epoch 4/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.3427 - accuracy: 0.8628\n",
      "Epoch 5/15\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 0.2818 - accuracy: 0.8794\n",
      "Epoch 6/15\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 0.2501 - accuracy: 0.8883\n",
      "Epoch 7/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.2310 - accuracy: 0.8883\n",
      "Epoch 8/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.2174 - accuracy: 0.8974\n",
      "Epoch 9/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.2043 - accuracy: 0.8989\n",
      "Epoch 10/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.1953 - accuracy: 0.9043\n",
      "Epoch 11/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.1921 - accuracy: 0.9005\n",
      "Epoch 12/15\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 0.1860 - accuracy: 0.9014\n",
      "Epoch 13/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.1801 - accuracy: 0.9054\n",
      "Epoch 14/15\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.1781 - accuracy: 0.9058\n",
      "Epoch 15/15\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 0.1742 - accuracy: 0.9029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x147b89550>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(6, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8216 - accuracy: 0.5890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8216090202331543, 0.5890295505523682]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: titles-59.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('titles-59.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=tf.keras.models.load_model('titles-59.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = ['Crime-Thriller', \n",
    "         'Graphic-Novels-Anime-Manga', \n",
    "         'Mind-Body-Spirit', \n",
    "         'Romance', \n",
    "         'Science-Fiction-Fantasy-Horror', \n",
    "         'Travel-Holiday-Guides']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = \"Wild\"\n",
    "step1 = clean_up(test1)\n",
    "step2 = tokenize(step1)\n",
    "step3 = stem_and_lemmatize(step2)\n",
    "step4 = remove_stopwords(step3)\n",
    "step5 = score(step4)\n",
    "\n",
    "\n",
    "predictions = new_model.predict([step5])\n",
    "np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\\\n",
    "A Bear Called Paddington - Graphic-Novels-Anime-Manga\\\n",
    "Batman Vol. 4 The War Of Jokes And Riddles (Rebirth) - Graphic-Novels-Anime-Manga\\\n",
    "Asterix and Cleopatra - Graphic-Novels-Anime-Manga\\\n",
    "World Atlas Explore the World - Travel-Holiday-Guides\\\n",
    "Lonely Planet Cuba (Travel Guide) - Travel-Holiday-Guides\\\n",
    "Wild: From Lost to Found on the Pacific Crest Trail - Travel-Holiday-Guides\\\n",
    "The Power of Now: A Guide to Spiritual Enlightenment - Mind-Body-Spirit\\\n",
    "The 7 Habits of Highly Effective People - Mind-Body-Spirit\\\n",
    "The Easy Way to Stop Smoking - Mind-Body-Spirit\\\n",
    "A Game of Thrones - Science-Fiction-Fantasy-Horror\\\n",
    "The Hobbit - Science-Fiction-Fantasy-Horror\\\n",
    "Fahrenheit 451 - Science-Fiction-Fantasy-Horror\\\n",
    "Pride and Prejudice - Romance\\\n",
    "The Notebook - Romance\\\n",
    "Fifty Shades of Grey - Romance\\\n",
    "The Girl with the Dragon Tattoo - Crime-Thriller\\\n",
    "The Shining - Crime-Thriller\\\n",
    "Misery - Crime-Thriller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020 Books\\\n",
    "The Glass Hotel Crime-Thriller (YES, thriller)\\\n",
    "It's Not All Downhill From Here Crime-Thriller (NO, Romance)\\\n",
    "Empire of Wild Science-Fiction-Fantasy-Horror (YES, Fantasy)\\\n",
    "Tiny Habits: The Small Changes That Change Everything Mind-Body-Spirit (YES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
